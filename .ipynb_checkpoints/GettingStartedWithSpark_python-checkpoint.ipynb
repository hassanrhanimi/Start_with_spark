{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f66183",
   "metadata": {},
   "source": [
    "# Getting Started With Spark using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a54c4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"http://spark.apache.org/images/spark-logo.png\" width=\"300\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f27598",
   "metadata": {},
   "source": [
    "Spark is an open source in-memory application framework for distributed data processing (cluster computing environment) and iterative analysis on big data. \n",
    "\n",
    "It's a multi-language (Scala, python, Java, R, SQL) engine for executing data engineering, data science, and machine learning on single-node machines or clusters. \n",
    "\n",
    "Spark is written in Scala, which compiles to Java bytecode, but you can write python code to communicate to the java virtual machine through a library called py4j. Python has the richest API (pyspark), but it can be somewhat limiting if you need to use a method that is not available or because of the slowness runing caused by latency associated with communicating back and forth to the JVM. An exception to this is the SparkSQL library, which has an execution planning engine that precompiles the queries.\n",
    "\n",
    "General recommandations :\n",
    "- If you need to write high-performance or specialized code, try doing it in scala.  \n",
    "- Use the \"out of the box\" methods available as much as possible and avoid overly frequent (iterative) calls to Spark methods. \n",
    "\n",
    "Apache Spark solves the problems encountered with MapReduce (The processing component of Hadoop) by keeping a substantial portion of the data required in-memory, avoiding expensive and time-consuming disk I/O.\n",
    "\n",
    "Learn more <a href=\"https://spark.apache.org/\">here</a> about spark.\n",
    "\n",
    "For Standalone use, please see <a href=\"https://phoenixnap.com/kb/install-spark-on-ubuntu\">this link</a> to install spark easily on Ubunto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e1352",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this notebook, we will go over the basics of Apache Spark and PySpark. We will start with giving some concept definitions, creating the SparkContext and SparkSession. We then create an RDD and apply some basic transformations and actions. Finally we'll present methods for creating Spark DataFrame & demonstrate the basics dataframes and SparkSQL and how to close the spark context ans session connections.\n",
    "\n",
    "by the time you finish reading, you will be able to:\n",
    "\n",
    "- Define RDD, DataFrame, Dataset, sparkContext, sparkSession\n",
    "- Create the SparkContext and SparkSession\n",
    "- Create an RDD and apply some basic transformations and actions on RDDs\n",
    "- Demonstrate the use of the basics Dataframes and SparkSQL\n",
    "- Methods for creating Spark DataFrame\n",
    "- Close spark context and session connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf0880",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "\n",
    "**RDD :** It stands for Resilient Distributed Datasets. It's Spark's primary data abstraction. It's a collection of fault tolerant `the capability to operate and to recover loss after a failure occurs beause of redundancy` elements partitioned across the cluster's nodes capable of receiving parallel operations. It's immutable,meaning that these databases cannot be changed once created.\n",
    "RDD's support text, sequence files, Avro, Parquet and Hadoop input format file types. RDD's also support local, Cassandra, H Base, HDFS, Amazon S3, and other file formats in addition to an abundance of relational and no SQL databases. RDD's are created :\n",
    "- Using an external or local file from a Hadoop supported file system such as HDFS, Cassandra, H Base or Amazon S3.\n",
    "- Applying the parallelize function to an existing collection in the driver program.\n",
    "- Applying a transformation on an existing RDD to create a new RDD\n",
    "\n",
    "RDD's provide resilience in Spark through immutability and caching. First RDDs are always recoverable as the data is immutable. Another essential Spark capability is the persisting or caching of a data set in memory across operations. The cache is fault tolerant and always recoverable. \n",
    "\n",
    "**SparkSQL** is a Spark module for structured data processing. You can interact with SparkSQL using SQL queries and the DataFrame API. Spark SQL uses the same execution engine to compute the result independently of which API or language you are using for the computation. \n",
    "\n",
    "**DataFrame :** is a collection of data organized into named columns. DataFrames are conceptually equivalent to a table in a relational database and similar to a data frame in R/Python, but with richer optimizations.\n",
    "DataFrames are built on top of the Spark RDD.\n",
    "DataFrames are highly scalable from a few kilobytes on a single laptop to petabytes (`10^6 GB`) on a large cluster of machines. DataFrames provide optimization and code generation through a Catalyst optimizer. \n",
    "\n",
    "**Dataset :** Datasets are the newest Spark data abstraction. Like RDDs and DataFrames, datasets provide an API to access a distributed data collection. Datasets are a collection of strongly typed Java Virtual Machine objects. Strongly typed means that datasets are typesafe, and the dataset’s datatype is made explicit\n",
    "during its creation. Datasets provide the benefits of both RDDs, such as lambda functions, type-safety, and SQL Optimizations from SparkSQL. Because datasets are strongly typed, APIs are currently only available in Scala and Java, which are statically typed languages. `Dynamically typed languages, such as Python and R, do not support dataset APIs`. \n",
    "\n",
    "**SparkContext :** is an entry point to Spark programming with RDD and to connect to Spark Cluster. It's used to programmatically create Spark RDD, accumulators and broadcast variables on the cluster. Since Spark 2.0 most of the functionalities (methods) available in SparkContext are also available in SparkSession. Its object `sc` is default available in spark-shell and it can be programmatically created using SparkContext class. \n",
    "\n",
    "**SparkSession :** Since Spark 2.0 SparkSession has been introduced and became an entry point to start programming with DataFrame and Dataset. It’s object `spark` is default available in spark-shell and it can be created programmatically using SparkSession builder pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a9199",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50de4fd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/mbg/anaconda3/lib/python3.8/site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/mbg/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: findspark in /home/mbg/anaconda3/lib/python3.8/site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6724404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pyspark to sys.path and initialize\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae7f2b",
   "metadata": {},
   "source": [
    "Check <a href=\"https://github.com/minrk/findspark\">this link</a> to understand what findspark.init() do exacteley."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60af35",
   "metadata": {},
   "source": [
    "## Creating Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5c4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe API session into Spark and create a session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acd16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create a SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"projectName\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b1b07",
   "metadata": {},
   "source": [
    "## RDD's and basic transformations & action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffefe0",
   "metadata": {},
   "source": [
    "**Create an RDD.**\n",
    "\n",
    "For demonstration purposes, we create an RDD here by calling sc.parallelize()\n",
    "We create an RDD which has integers from 1 to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a38ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,31)\n",
    "# print first element of iterator\n",
    "print(data[0])\n",
    "\n",
    "#Create the RDD (a distribueted dataset with 4 partitions)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "xrangeRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b5425",
   "metadata": {},
   "source": [
    "**Transformations**\n",
    "\n",
    "A transformation is an operation on an RDD that results in a new RDD. The transformed RDD is generated rapidly because the new RDD is lazily evaluated, which means that the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called. In this transformation, we reduce each element in the RDD by 1. Note the use of the lambda function. We also then filter the RDD to only contain elements <10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5a2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d165d1",
   "metadata": {},
   "source": [
    "**Actions**\n",
    "\n",
    "A transformation returns a result to the driver. We now apply the collect() action to get the output from the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d15dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83522f",
   "metadata": {},
   "source": [
    "**Caching Data**\n",
    "\n",
    "This simple example shows how to create an RDD and cache it. Notice the 10x speed improvement! If you wish to see the actual computation time, browse to the Spark UI...it's at host:4040 or see the link bellow. You'll see that the second calculation took much less time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0d95c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2a6144e9d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f00478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt1:  1.0804994106292725\n",
      "dt2:  0.2651362419128418\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "test = sc.parallelize(range(1,50000),4)\n",
    "test.cache()\n",
    "\n",
    "t1 = time.time()\n",
    "# first count will trigger evaluation of count *and* then cache\n",
    "count1 = test.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"dt1: \", dt1)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "# second count operates on cached data only\n",
    "count2 = test.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"dt2: \", dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc8dd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b0d97",
   "metadata": {},
   "source": [
    "Let's now create an RDD with integers from 1-50. Apply a transformation to multiply every number by 2, resulting in an RDD that contains the first 50 even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7021a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = range(1, 51)\n",
    "numbers_RDD = sc.parallelize(numbers, 4)\n",
    "even_numbers_RDD = numbers_RDD.map(lambda x: x*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ac48ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_numbers_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cf986",
   "metadata": {},
   "source": [
    "## DataFrames and SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b850de",
   "metadata": {},
   "source": [
    "In order to work with the extremely powerful SQL engine in Apache Spark, you will need a **Spark Session**. We have created an instance above, let's verify that spark session is still active. Feel free to click on the \"Spark UI\" button to explore the Spark UI elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22f63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2a6144e9d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30785fe3",
   "metadata": {},
   "source": [
    "**Create Your First DataFrame**\n",
    "\n",
    "You can create a structured data set (much like a database table) in Spark. Once you have done that, you can then use powerful SQL tools to query and join your dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5c43e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    73  100    73    0     0    366      0 --:--:-- --:--:-- --:--:--   366\n"
     ]
    }
   ],
   "source": [
    "# Download the data first into a local `people.json` file\n",
    "!curl https://raw.githubusercontent.com/hassanrhanimi/Start_with_spark/main/people.json > people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3447c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset into a spark dataframe using the `read.json()` function\n",
    "df = spark.read.json(\"people.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "609a6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8b1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use sql, let's first register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326b24c",
   "metadata": {},
   "source": [
    "**Explore the data using DataFrame functions and SparkSQL**\n",
    "\n",
    "In this section, we explore the datasets using functions both from dataframes as well as corresponding SQL queries using sparksql. Note the different ways to achieve the same task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1b9f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select and show basic data columns\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c140ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform basic filtering\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022e5e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|  30|    1|\n",
      "|null|    1|\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|  30|    1|\n",
      "|null|    0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perfom basic aggregation of data\n",
    "df.groupBy(\"age\").count().show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f4e29",
   "metadata": {},
   "source": [
    "Similar to the people.json file, now read the people2.json file into the notebook, load it into a dataframe and apply SQL operations to determine the average age in our people2 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14242478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   136  100   136    0     0   1416      0 --:--:-- --:--:-- --:--:--  1416\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/hassanrhanimi/Start_with_spark/main/people2.json > people2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9679bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('people2.json')\n",
    "# As before, let's first register the DataFrame as a SQL temporary view be able to use sql, \n",
    "df.createTempView(\"people2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a74d86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    24.8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT AVG(age) from people2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19b6dd",
   "metadata": {},
   "source": [
    "## Methods for creating Spark DataFrame\n",
    "\n",
    "There are three ways to create a DataFrame in Spark by hand:\n",
    "\n",
    "1. Create a list and parse it as a DataFrame using the createDataFrame() method from the SparkSession.\n",
    "\n",
    "2. Convert an RDD to a DataFrame using the toDF() method.\n",
    "\n",
    "3. Import a file into a SparkSession as a DataFrame directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156985d6",
   "metadata": {},
   "source": [
    "### 1. Create DataFrame from a list of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef1c4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate toy data using a dictionary list\n",
    "data = [{\"Category\": 'A', \"ID\": 1, \"Value\": 121.44, \"Truth\": True},\n",
    "        {\"Category\": 'B', \"ID\": 2, \"Value\": 300.01, \"Truth\": False},\n",
    "        {\"Category\": 'C', \"ID\": 3, \"Value\": 10.99, \"Truth\": None},\n",
    "        {\"Category\": 'E', \"ID\": 4, \"Value\": 33.87, \"Truth\": True}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84c7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import and create a SparkSession : Created already above\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3d0f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a DataFrame using the createDataFrame on the `spark`\n",
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b42d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Truth: boolean (nullable = true)\n",
      " |-- Value: double (nullable = true)\n",
      "\n",
      "+--------+---+-----+------+\n",
      "|Category| ID|Truth| Value|\n",
      "+--------+---+-----+------+\n",
      "|       A|  1| true|121.44|\n",
      "|       B|  2|false|300.01|\n",
      "|       C|  3| null| 10.99|\n",
      "|       E|  4| true| 33.87|\n",
      "+--------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print the schema and view the DataFrame in table format\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d6b3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----+------+\n",
      "|Category| ID|Truth| Value|\n",
      "+--------+---+-----+------+\n",
      "|       E|  4| true| 33.87|\n",
      "|       A|  1| true|121.44|\n",
      "+--------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#option 1 of use : Using Domain-Specific Queries\n",
    "df.filter(df.Truth==True).sort(df.Value).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8448414d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----+------+\n",
      "|Category| ID|Truth| Value|\n",
      "+--------+---+-----+------+\n",
      "|       E|  4| true| 33.87|\n",
      "|       A|  1| true|121.44|\n",
      "+--------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#option 2 of use : Using SQL Queries\n",
    "df.createOrReplaceTempView('table')\n",
    "spark.sql('''SELECT * FROM table \n",
    "                WHERE Truth=true \n",
    "                ORDER BY Value ASC''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ae7f8",
   "metadata": {},
   "source": [
    "### 2. Create DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d12cbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import and create a SparkContext: Created already above\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf().setAppName(\"projectName\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86cee75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Generate an RDD from the created data. Check the type to confirm the object is an RDD:\n",
    "rdd = sc.parallelize(data)\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12bd0e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Call the toDF() method on the RDD to create the DataFrame. Test the object type to confirm:\n",
    "df = rdd.toDF()\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce75c98",
   "metadata": {},
   "source": [
    "### 3. Create DataFrame from Data sources\n",
    "\n",
    "Spark can handle a wide array of external data sources to construct DataFrames. The general syntax for reading from a file is:\n",
    "\n",
    "spark.read.format('<data source>').load('<file path/file name>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae789fb6",
   "metadata": {},
   "source": [
    "#### 3.1. Creating from CSV file\n",
    "\n",
    "Create a Spark DataFrame by directly reading from a CSV file:\n",
    "\n",
    "`df = spark.read.csv('<file name>.csv')`\n",
    "    \n",
    "Read multiple CSV files into one DataFrame by providing a list of paths:\n",
    "\n",
    "`df = spark.read.csv(['<file name 1>.csv', '<file name 2>.csv', '<file name 3>.csv'])`\n",
    "\n",
    "By default, Spark adds a header for each column. If a CSV file has a header you want to include, add the option method when importing:\n",
    "\n",
    "`df = spark.read.csv('<file name>.csv').option('header', 'true')`\n",
    "\n",
    "Individual options stacks by calling them one after the other. Alternatively, use the options method when more options are needed during import:\n",
    "\n",
    "`df = spark.read.csv('<file name>.csv').options(header = True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca3b47",
   "metadata": {},
   "source": [
    "#### 3.2. Creating from TXT file\n",
    "\n",
    "Create a DataFrame from a text file with:\n",
    "\n",
    "`df = spark.read.text('<file name>.txt')`\n",
    "\n",
    "The csv method is another way to read from a txt file type into a DataFrame. For example:\n",
    "\n",
    "`df = spark.read.option('header', 'true').csv('<file name>.txt')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b4662e",
   "metadata": {},
   "source": [
    "#### 3.3. Creating from JSON file\n",
    "\n",
    "Make a Spark DataFrame from a JSON file by running:\n",
    "\n",
    "`df = spark.read.json('<file name>.json')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9aa7ec",
   "metadata": {},
   "source": [
    "#### 3.4. Creating from an XML file\n",
    "\n",
    "XML file compatibility is not available by default. Install the dependencies to create a DataFrame from an XML source.\n",
    "\n",
    "1. Download the Spark XML dependency. Save the .jar file in the Spark jar folder.\n",
    "\n",
    "2. Read an XML file into a DataFrame by running:\n",
    "\n",
    "`df = spark.read\\\n",
    "            .format('com.databricks.spark.xml')\\\n",
    "            .option('rowTag', 'row')\\\n",
    "            .load('test.xml')`\n",
    "            \n",
    "Change the rowTag option if each row in your XML file is labeled differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb586f2f",
   "metadata": {},
   "source": [
    "#### 3.5 Create DataFrame from RDBMS Database\n",
    "\n",
    "Reading from an RDBMS requires a driver connector. The example goes through how to connect and pull data from a MySQL database. Similar steps work for other database types.\n",
    "\n",
    "1. Download the MySQL Java Driver connector. Save the .jar file in the Spark jar folder.\n",
    "\n",
    "2. Run the SQL server and establish a connection.\n",
    "\n",
    "3. Establish a connection and fetch the whole MySQL database table into a DataFrame:\n",
    "\n",
    "`df = spark.read\\\n",
    "            .format('jdbc')\\\n",
    "            .option('url', 'jdbc:mysql://localhost:3306/db')\\\n",
    "            .option('driver', 'com.mysql.jdbc.Driver')\\\n",
    "            .option('dbtable','new_table')\\\n",
    "            .option('user','root')\\\n",
    "            .load()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc1bdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the SparkSession \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33d458fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the sparkContext \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95129fb",
   "metadata": {},
   "source": [
    "**Sources** : \n",
    "- IBM lab skills on coursera (data engineering specialization) \n",
    "- Apache spark foundation, \n",
    "- https://phoenixnap.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
